# 搜索引擎算法设计

### 现有引擎的问题

基于现有的数据库结构，目前的搜索引擎算法有两个局限：

- 仅支持对标题的搜索，无法支持对blog_question-content和blog_answer的匹配
- 搜索算法采用string match模式，复杂度较高

-------

**针对第一个问题**，现在数据库里存放模式是：两张表 blog_question和blog_answer.

**Blog questions:**

<img src="/Users/huangpengxiang/Library/Application Support/typora-user-images/Screen Shot 2022-04-29 at 20.33.32.png" alt="Screen Shot 2022-04-29 at 20.33.32" style="zoom:30%;" />

**Blog answer:**

<img src="/Users/huangpengxiang/Library/Application Support/typora-user-images/Screen Shot 2022-04-29 at 20.56.06.png" alt="Screen Shot 2022-04-29 at 20.56.06" style="zoom:30%;" />

如果存储的模式不发生改变的话，那么需要对两张表的内容同时进行搜索，I/O的读写次数很高，或者采用join的办法合并表格，但是合并表格的开销同样不小。

另外，content内容返回的是html格式，需要对html格式进行解码操作，去除html和css标识符，得到数据字符串。

**针对第二个问题，** 目前的搜索算法如下：

![Screen Shot 2022-04-29 at 23.12.03](/Users/huangpengxiang/Library/Application Support/typora-user-images/Screen Shot 2022-04-29 at 23.12.03.png)

在全局搜索范围下，将数据库里的每一个关键词（key）与标题分词后所出现的关键词进行匹配，如果数据库的关键词（key）**包含**了标题的关键词，其相似度加1，最终返回一个相似度的排序数组，选取相似度高所对应的blog

这种算法的复杂度是O(N * k), N是数据库里的所有关键词，k是标题的关键词。如果后期将blog和blog answer的内容考虑进来的话，代表每一次全局搜索的操作都需要将数据库里所有的字符全部扫一遍，这是一个相当大的搜索量。

----------

### 解决方案

基于第一个问题，如果需要实现对blog content和blog answer的搜索，则需要对这两个分别进行扫描，或者在初始化搜索引擎的时候将其存入对应的数据结构。同时，由于这两个的存放形式是HTML格式，需要对其进行解码操作。

HTML格式的解码操作不复杂，python3里有自带的HTML.parser对其进行解码，

所以主要的问题还是数据结构的存放问题，和第二个问题本质都是构建一个搜索引擎。

#### 搜索引擎的构建

解决这一问题的思路是构建**反向索引(inverted index).** 其具体流程如下：

首先如果采用正向搜索，举例

| 1    | How to build kernel     |
| ---- | ----------------------- |
| 2    | How to learn C++        |
| 3    | How about my code below |

如果用户输入`how`， 那么这样的搜索引擎需要把数据库里(1,3)的数据全部扫一遍，工作量很大

所以我们构建一个反向索引(inverted index),形如下：

| How   | {1,2,3} |
| ----- | ------- |
| Build | {2}     |
| Learn | {2}     |
| code  | {3}     |
| ..    | ..      |

那么用户在搜索How的时候，可以直接返回blog_id = {1,2,3},这样的搜索效率高，不需要对数据库进行扫描，缺点是

- 需要在每次创建一个blog的时候，需要对blog的所有内容进行扫描并且分词，然后将每个词插入到对应的索引表当中去，同时需要创建和维护索引表
- 搜索的复杂度虽然降低（因为一张索引表的词理论上常用的不会超过1w个，并且除去那些the, a, an等词),但是一张索引表的消耗的空间同样很大
- 创建blog的开销增大，但是此过程可以对用户不透明

这样一张索引表的创建和维护流程如下：

**对数据库已有的数据建表：**

创建一张表格存在数据库里，形如下：

| index id | key word | key word in title                      | Key word in content                       | Update time |
| -------- | -------- | -------------------------------------- | ----------------------------------------- | ----------- |
| 1        | How      | {1,2}, {tilte_id, number of occurance} | {12, 3},  {tilte_id, number of occurance} | 0           |
| 2        | Build    | {1,2}, {tilte_id, number of occurance} | {12, 3},  {tilte_id, number of occurance} | 1           |



**搜索：**

1. 创建一个空的字典数组，里面存放**{文章id（key），文章相似度 （value）}** 
2. 线性搜索关键词，如果搜索命中,则往字典里面添加{文章id，文章相似度}，如果字典里存在文章id，那么修改文章相似度。如果字典里没有文章id，则为其创建一个。在title和在content里的命中具有不同的占比，比例可设置为3:1，如果在tilte里命中，可以相似度加3，在content里命中，则相似度+1。 如果整个表里无一命中，则返回空，表示搜索不到结果。

3. 根据字典里的value值排序，选取前面几个相似度高的文章id，返回给前端

理论上每次的搜索复杂度是O(N)，N是索引表的长度，如果索引表长度较长，则可以考虑建立**二级索引**。比如一级索引根据A-Z的首字母排列，二级索引储存所有的关键词。





**维护索引表：**

1. 索引表的增加。 当有新的blog输入的时候，需要对blog的内容进行分词切割和存储。分词的时候需要除去对应的a，an，she，he等词，保留key word。然后将所有的关键词存入索引表中
2. 索引表的修改，当有blog加入的时候，对应的关键词已经在索引表中出现，这个时候需要向索引表中添加对应的tuple进行修改
3. 索引表的删除，当有blog删除的时候，需要修改（暂时不支持删除，所以暂时不考虑）

==问题：每次对关键词进行增加和修改的时候，每一个关键词都需要扫描一整张索引表来判断是否在里面，所以在考虑是否需要建立一个二级索引表来优化算法，或者对索引表采用其他的排序方式来适应一个二分查找的算法==



**分词**

分词是比较复杂的一部分，一般采用**TF-IDF**的原则，选取其中重要的词汇，也可以实现建立一个表格储存所有的不重要词汇，例如[the, a, an, He, She, It]等等

Github也有一些分词器可以直接拿来使用：

https://github.com/stanfordnlp/CoreNLP.git